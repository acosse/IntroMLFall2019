{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises, Week 2: First intuition on parameter estimation, Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning goals: \n",
    "    \n",
    "   - Be able to understand the difference between Bayesian and Frequentist approaches to statistics\n",
    "   - Be able to understand the utility of parameter estimation in simple machine learning examples\n",
    "   - Be able to learn simple regression models and understand the connection between Bayesian priors and regularization "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__1. A simple two classes dataset__ To understand the interest of parameter inference in machine learning we will generate data from two unknown distributions (representing distinct sets of behaviors) and then try to approximately recover the underlying distributions in order to be able to classify new samples in one of the two groups."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__1a__ As a first step, generate two one dimensional gaussian distributions with mean resp. -5 and 5 and std 2. From each distribution, generate a set of 40 samples $\\left\\{x_i\\right\\}_i$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# put your code here\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__1b__ Now that you have two sets of prototypes $\\left\\{x_{1,i}\\right\\}$ and $\\left\\{x_{2,i}\\right\\}$, we will associate to the first set the label $t=1$ and to the second, the label $t=2$. We will then learn the parameters $\\mu_1$ and $\\sigma_1$ as well as $\\mu_2$ and $\\sigma_2$ to then use them as a model to do prediction regarding the class ($1$ or $2$) of new data points. \n",
    "\n",
    "- Start by considering each set of points independently (we pretend that we know they come from distinct distributions). For each problem independently, write down the data log likelihood and compute the parameters $\\mu_1$ and $\\sigma_1$.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# put your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__1c__ In general we do not have access to the two point clouds separately but as a whole. group the points together, in a single dataset $\\left\\{x_{1,i}\\right\\}_i\\cup \\left\\{x_{2,i}\\right\\}_i$. Group those points in two clusters that you can choose at random (we don't assume any prior information now). The only information we have is that we believe there two clusters. The distribution in this case reads as\n",
    "\n",
    "$$p(x_i|\\mu_1,\\mu_2, \\sigma_1, \\sigma_2) = \\frac{1}{\\sqrt{2\\pi \\sigma_1^2}} e^{-\\frac{(x_i - \\mu)^2}{2\\sigma^2_1}} \\pi_1 + \\frac{1}{\\sqrt{2\\pi \\sigma_1^2}} e^{-\\frac{(x_i - \\mu)^2}{2\\sigma^2_1}} \\pi_2$$\n",
    "\n",
    "Where $\\pi_1$ and $\\pi_2$ indicates how likely the point is to have been generated from either of the two clusters.\n",
    "\n",
    "- Start by showing that for $N$ data points, the log likelihood reads as\n",
    "\n",
    "\\begin{align}\n",
    "\\log p(x_i|\\mu_1,\\sigma, \\pi) = \\sum_{i=1}^N \\log\\left(\\pi_1\\mathcal{N}(x_i|\\mu_1,\\sigma_1) +\\pi_1\\mathcal{N}(x_i|\\mu_1,\\sigma_1) \\right)\n",
    "\\end{align}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A popular approach to learn $\\mu_1,\\mu_2, \\sigma_1, \\sigma_2$ as well as $\\pi_1,\\pi_2$ when we don't have information on the underlying model is to use the EM algorithm. Without entering in the details, this algorithm iterates between the following steps.\n",
    "\n",
    "- Initialize the mean, variance and mixing coefficients $\\pi$ and evaluate the log likelihood\n",
    "\n",
    "- (E-step) Evaluate the responsabilities (proability that point $x_i$ comes from distribution $1$ or $2$)\n",
    "\n",
    "\\begin{align}\n",
    "r_{i,\\ell} = \\frac{\\pi_{\\ell}\\mathcal{N}(x_i|\\mu_\\ell,\\sigma_\\ell)}{\\sum_{k=1}^2 \\pi_k \\mathcal{N}(x_i|\\mu_k, \\sigma_k)}, \\quad \\ell=1, 2.\n",
    "\\end{align}\n",
    "\n",
    "- (M-Step). Restime the parameters\n",
    "\n",
    "    \\begin{align}\n",
    "    \\mu^{new} &= \\frac{1}{N_k} \\sum_{i=1}^N x_kr_{i,k}\\\\\n",
    "    \\sigma_k^{new} & = \\frac{1}{N_k} \\sum_{i=1}^N r_{i,k} (x_i - \\mu_k)^2\n",
    "    \\pi_k^{new}& = \\frac{N_k}{N}\n",
    "    \\end{align}\n",
    "    \n",
    "    where $N_k = \\sum_{i=1}^N r_{i,k}$\n",
    "\n",
    "- Stopping criterion, check the likelihood function and or look at the evolution of the parameters. Stop when the likelihood stops increasing and or when the parameters stop updating.\n",
    "\n",
    "Implement the EM algorithm on the $2$ clusters distributions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# put your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__2. Complementing views on regression__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, we will see that regularizing a regression model can be understood as adding a prior on the regression coefficients, in a Bayesian framework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__2a. Linear Regression: First intuition and statistical interpretation__ As a very simple introduction to regression, consider the linear model\n",
    "\n",
    "\\begin{align}\n",
    "y_i = \\beta_1 x_i + \\beta_0\n",
    "\\end{align}\n",
    "\n",
    " - This equation defines a line. Using linspace from numpy generate 100 points according to that equation for parameters $\\beta_1$ and $\\beta_0$ of your own choice. Now that you got to try those functions in HW1, plot the line and the points using pyplot.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# put your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Add an independent Gaussian perturbation on your samples $x_i$. So that you now have pairs $(x_i+\\varepsilon_i, t_i)$. Take the mean and variance of the $\\varepsilon_i$ to be respectively $0$ and something sufficiently small, let us say $0.1$. Plot the new samples on top of the line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# put your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__2b__ Optimization perspective. We would like to recover the line (i.e. the underlying model) from the noisy samples. Thinks of it as having some data on some ongoing phenomenon whose value is either growing or decreasing depending on your choice of the $\\beta_1,\\beta_0$. And we would like to know how the phenomenon will evolve in the future. To learn $\\beta_1, \\beta_0$, a reasonable approach is to find the coefficients that minimize the sum of the squared residuals \n",
    "\n",
    "\\begin{align}\n",
    "\\sum_{i=1}^N |t_i - \\left(\\beta_1x_i + \\beta_0\\right)|^2\n",
    "\\end{align}\n",
    "\n",
    "The idea is known as linear regression. Apply your gradient descent algorithm to that function with a sufficiently small learning rate. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now consider a higher dimensional model\n",
    "\n",
    "\\begin{align}\n",
    "\\sum_{i=1}^N |t_i - \\left(a_1x_{1,i} + a_2x_{2,i}+ b_0\\right)|^2\n",
    "\\end{align}\n",
    "\n",
    "- Using scatter3, plot the prototypes encoded in the data matrix below.\n",
    "\n",
    "- Generate the labels for $a_1 = 12, a_2 = -5$ and $b=1$\n",
    "- Generate the labels for $a_1 = 102, a_2 = -50$ and $b=1$\n",
    "\n",
    "__2b.1__Compare those labels. What do you notice? Why ?__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "x1 = np.linspace(0, 10, 50)\n",
    "x2 = 2*np.linspace(0, 10, 50)\n",
    "\n",
    "X_training = np.zeros((50,2));\n",
    "\n",
    "X_training[:,0] = x1\n",
    "X_training[:,1] = x2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__2b.2__ What can we say about the rank of $X_training$ ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__2b.3__ \n",
    "\n",
    "- Generate the labels for $a_1 = 1002, a_2 = -500$ and $b=1$? How do those labels look like?\n",
    "- Let us assume that from the training data stored in $X_training$, we have learned the two models $a_1 = 1002, a_2 = -500$ and $b=1$ on the one hand and $a_1 = 12, a_2 = -5$ and $b=1$ on the other. Assume that we are now given some new data which is not perfectly aligned to the training data but a little noisy. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "x1 = np.linspace(0, 10, 50)+0.1\n",
    "x2 = 2*np.linspace(0, 10, 50) + np.random.normal(0,.1,50)\n",
    "\n",
    "Xtest = np.zeros((50,2));\n",
    "\n",
    "Xtest[:,0] = x1\n",
    "Xtest[:,1] = x2\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict the labels for the new points using the models $a_1 = 1002, a_2 = -500, b=1$ and $a_1 = 12, a_2 = -5, b=1$. How do they compare to each other?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To avoid risking huge difference in the behaviors of models that we learn on our dataset, we reduce tje freedom in the choice of the parameters by adding a regularization term. In the example above, a $1D$ model would be perfectly enough to describe the data and we will therefore penalize the complexity by looking for a model with either small coefficients $a_1,a_2$ or require one of those coefficients to be zero for example. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__3 Regularization.__ \n",
    "\n",
    "__3a__ Consider the model \n",
    "\n",
    "\n",
    "\\begin{align}\n",
    "\\sum_{i=1}^N |t_i - \\left(a_1x_{1,i} + a_2x_{2,i}+ b_0\\right)|^2 + \\lambda \\left(|a_2|^2 + |a_1|^2\\right)\n",
    "\\end{align}\n",
    "\n",
    "with $\\lambda=100$ let's say. Imagine that you run a gradient descent algorithm on that loss, which one of the models $a_1 = 1002, a_2 = -500, b=1$ and $a_1 = 12, a_2 = -5, b=1$ are you more likely to recover and why?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__3b__ Compare the behavior of the two models using the functions LinearRegression and RidgeRegression from scikit-learn, on the data encoded in the matrix X_train. Check the parameters returned by each approach."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
